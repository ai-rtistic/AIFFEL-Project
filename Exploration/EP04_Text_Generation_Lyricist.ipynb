{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "altered-closing",
   "metadata": {},
   "source": [
    "# Exploration4 . Text Generation. 작사가 만들기\n",
    "\n",
    "\n",
    "작사가는 노래의 가사를 전문적으로 만들어내는 사람이다.\n",
    "\n",
    "\n",
    "Text Generation 을 활용하여 작사를 할 수 있을까\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-avenue",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/84179578/126740374-78072ca8-87a0-4667-90b2-b0a8fa1801e0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-sampling",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기\n",
    "\n",
    "\n",
    "여러 가사들에 대한 데이터는 `EP04_data` 파일에 `lyrics` 파일내에 저장되어있다. 파일을 읽어서 가사 데이터를 불러오자.\n",
    "\n",
    "`glob` 모듈을 활용하면 파일을 읽어오는 작업을 하기가 편리하다.  \n",
    "`glob` 을 포함하여 이번 프로젝트에서 사용할 여러 모듈들을 불러오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faced-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = 'EP04_data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-absolute",
   "metadata": {},
   "source": [
    "불러온 모든 `txt` 파일을 `raw_corpus` 리스트에 문장 단위로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "composite-metabolism",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[:Nicki Minaj]', 'Young money [Verse 1: Jason Derulo] (?) thousand different favors', 'I wish that I could (?)', \"No i ain't got no dinner plans\", 'So you should bring all your friends', 'I swear to (?) that you my type Noooo [Hook :Jason Derulo]', 'Shimmy shimmy she, shimmy she, shimmy yah', 'Swalla-la-la, swalla-la-la, swalla-la-la', 'Shimmy shimmy she, shimmy she, shimmy yah', 'Swalla-la-la, swalla-la-la, swalla-la-la']\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-wayne",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제\n",
    "\n",
    "우리가 불러온 문장 중 필요한 문장은 __가사__ 부분이다. \n",
    "따라서 다음과 같은 필요하지않은 부분은 제외한다.\n",
    "- 공백인 문장\n",
    "- 가수 파트 구분을 위해 대괄호가 포함된 부분\n",
    "  - `'Young money [Verse 1: Jason Derulo] (?) thousand different favors'` 와 같이 파트 구분이 되고 앞뒤 내용이 이어지지 않는 문장이 많음\n",
    "- 중복되는 문장\n",
    "\n",
    "\n",
    "위 부분은 제외한 문장을 새로운 리스트 `corpus` 에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boolean-publisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 데이터 크기 : 187088\n",
      "공백인 문장, 대괄호 포함된 문장 제외한 데이터 크기 : 174446\n",
      "중복되는 문장 제외한 데이터 크기 : 117114\n"
     ]
    }
   ],
   "source": [
    "corpus= []\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue           # 길이가 0인 문장 제외\n",
    "    if ('[' and ']') in sentence: continue    # 대괄호가 들어가는 문장 제외\n",
    "    \n",
    "    corpus.append(sentence)\n",
    "\n",
    "\n",
    "print(f'원래 데이터 크기 : {len(raw_corpus)}')\n",
    "print(f'공백인 문장, 대괄호 포함된 문장 제외한 데이터 크기 : {len(corpus)}')\n",
    "\n",
    "corpus = list(set(corpus))          # 중복불가한 set의 특성을 이용하여 중복되는 문장 제외 후 다시 리스트로 변환\n",
    "print(f'중복되는 문장 제외한 데이터 크기 : {len(corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-morocco",
   "metadata": {},
   "source": [
    "공백인 문장, 대괄호를 포함한 문장, 중복된 문장이 제외된 것을 확인 할 수 있다.  그 과정을 완료한 후 데이터 수는 117114 개 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-butterfly",
   "metadata": {},
   "source": [
    "## 3.토큰화\n",
    "\n",
    "다음과 같은 순서로 문장을 일정한 기준으로 쪼개는 토큰화 (Tokenize) 를 진행한다.  \n",
    "- 1. 소문자로 바꾸고, 양쪽 공백을 지움\n",
    "- 2. 특수문자 양쪽에 공백을 넣음\n",
    "- 3. 여러개의 공백은 하나의 공백으로 바꿈\n",
    "- 4. `a-zA-Z?.!,¿`가 아닌 모든 문자를 하나의 공백으로 바꿈\n",
    "- 5. 다시 양쪽 공백을 지움\n",
    "- 6. 문장 시작에는 `<start>`, 끝에는 `<end>`를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "secret-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()                     # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)   # 4\n",
    "    sentence = sentence.strip()                             # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>'             # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "casual-narrative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> shoo be doo wa wa , dead on it <end>',\n",
       " '<start> that girl i can t take her <end>',\n",
       " '<start> you don t even gotta tell em <end>',\n",
       " '<start> go rimbaud go rimbaud go rimbaud <end>',\n",
       " '<start> the girl that now is dead <end>',\n",
       " '<start> i can live , i can breathe <end>',\n",
       " '<start> red white and blue , <end>',\n",
       " '<start> and i know they love it . <end>',\n",
       " '<start> yeah ! make that change ! <end>',\n",
       " '<start> soaked in soul <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    \n",
    "    \n",
    "    # 데이터 정제\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus_tokenized.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인\n",
    "corpus_tokenized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-valuable",
   "metadata": {},
   "source": [
    "토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용한다.  \n",
    "\n",
    "15000단어를 기억할 수 있는 tokenizer 생성한다. 그에 포함되지 않는 단어들은 `<unk>` 으로 나타낸다.  \n",
    "\n",
    "또한, 지나치게 긴 문장들은 다른 데이터들이 과도한 padding 을 가지게 하므로 토큰의 개수가 15개를 초과하는 문장은 제외한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "endless-hypothesis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 5169   27 ...    0    0    0]\n",
      " [   2   17   85 ...    0    0    0]\n",
      " [   2    7   35 ...    0    0    0]\n",
      " ...\n",
      " [   2 5413    6 ...    0    0    0]\n",
      " [   2  328   22 ...    0    0    0]\n",
      " [   2 1282  907 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f918c7d7cd0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,         # 15000개 단어를 기억할 수 있음\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"        # 포함되지 않는 단어는 <unk> 으로 표현\n",
    "    )\n",
    "    \n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  # 토큰 15개 초과 제외\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-penetration",
   "metadata": {},
   "source": [
    "만들어진 tokenizer 을 보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opponent-product",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-brisbane",
   "metadata": {},
   "source": [
    "다음으로, tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성한다. 마지막 토큰은 `<end>`가 아니라 `<pad>`일 가능성이 높다.\n",
    "\n",
    "tensor에서 `<start>`를 잘라내서 타겟 문장을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "classified-custody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 5169   27  958 2152 2152    4  405   20   11    3    0    0    0]\n",
      "[5169   27  958 2152 2152    4  405   20   11    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # 소스문장 생성 \n",
    "\n",
    "tgt_input = tensor[:, 1:]  # 타켓 문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-token",
   "metadata": {},
   "source": [
    "## 4. 평가 데이터셋 분리\n",
    "\n",
    "`sklearn.model_selection.trian_test_split` 을 이용해 training set 과 test set 을 분리한다.\n",
    "\n",
    "이때 `test_size` 파라미터를 통해 test set 의 비율은 전체 데이터의 20% 로 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aware-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,       # 데이트셋 비율\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)     # 결과를 일정하게 보여주기위해 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lesser-insider",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (93691, 14)\n",
      "Target Train: (93691, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-burden",
   "metadata": {},
   "source": [
    "학습 데이터 개수가 93691 개인 것을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-shoulder",
   "metadata": {},
   "source": [
    "## 5. 모델 학습\n",
    "\n",
    "tf.keras.Model을 Subclassing하는 방식으로 모델을 만들 것이다.  \n",
    "\n",
    "모델은 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "broken-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-general",
   "metadata": {},
   "source": [
    "`embedding_size` 는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기이다.\n",
    "이번 예제에서는 `embedding_size` 를 256으로 지정하였다.   \n",
    "\n",
    "`hidden_size` 는 LSTM 레이어의 hidden state 의 차원수이다. \n",
    "이번 예제에서는 `hidden_size` 는 2048 로 지정하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "skilled-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-venue",
   "metadata": {},
   "source": [
    "epochs 값을 10으로 지정하여 모델을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "legislative-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2928/2928 [==============================] - 648s 213ms/step - loss: 3.7757\n",
      "Epoch 2/10\n",
      "2928/2928 [==============================] - 634s 217ms/step - loss: 3.0373\n",
      "Epoch 3/10\n",
      "2928/2928 [==============================] - 635s 217ms/step - loss: 2.7102\n",
      "Epoch 4/10\n",
      "2928/2928 [==============================] - 635s 217ms/step - loss: 2.3711\n",
      "Epoch 5/10\n",
      "2928/2928 [==============================] - 635s 217ms/step - loss: 2.0130\n",
      "Epoch 6/10\n",
      "2928/2928 [==============================] - 634s 217ms/step - loss: 1.6865\n",
      "Epoch 7/10\n",
      "2928/2928 [==============================] - 634s 216ms/step - loss: 1.4192\n",
      "Epoch 8/10\n",
      "2928/2928 [==============================] - 634s 216ms/step - loss: 1.2291\n",
      "Epoch 9/10\n",
      "2928/2928 [==============================] - 633s 216ms/step - loss: 1.1131\n",
      "Epoch 10/10\n",
      "2928/2928 [==============================] - 634s 216ms/step - loss: 1.0463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9188074a50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-executive",
   "metadata": {},
   "source": [
    "epoch 10번을 실행한 결과 loss가 1.0463 까지 떨어지는 것을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-saying",
   "metadata": {},
   "source": [
    "## 6. 모델 평가\n",
    "\n",
    "작문을 시켜보고 어떤지 직접 평가해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accredited-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "light-concrete",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> girl , you got it <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> girl\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "southern-southwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a lie <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "informal-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> if you want to cross the bridge , my sweet <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> if\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "median-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i was born to make you happy <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i was\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "professional-raising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the greatest thing to me . <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you are\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-essay",
   "metadata": {},
   "source": [
    "위와 같이 여러 단어를 통해 모델을 실행시킨 결과 실제 가사에 나올 법한 __sweeeeet__ 한 가사들이 생성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-pollution",
   "metadata": {},
   "source": [
    "## 프로젝트 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-fifty",
   "metadata": {},
   "source": [
    "많은 가사들을 불러와 데이터를 전처리하였다.   \n",
    "- 가수 파트를 구별하는 대괄호가 포함된 문장 삭제\n",
    "- 중복되는 문장 삭제\n",
    "- 15000개의 단어를 기억할 수 있는 tokenizer 생성\n",
    "- 토큰화 후 토큰 개수 15개 이상인 문장 삭제\n",
    "\n",
    "그것을 이용해 모델을 학습시켜 가사를 생성해내는 모델을 만들었다.   \n",
    "\n",
    "파라미터 값을 데이터에 알맞게 설정하여 모델을 학습시켰다.\n",
    "\n",
    "학습시킨 모델이 `i was born to make you happy`, `you are the greatest thing to me` 와 같이 굉장히 감성적인 가사를 쓸 수 있는 것을 확인하였다.  \n",
    "\n",
    "하지만 여전히 문장 문법이 어색하거나, 내용이 어색한 문장도 종종 생성되었다.  \n",
    "\n",
    "학습시키는 데이터 수를 늘리면 이러한 문제가 줄어들고 점점 사람이 만들어내는 가사와 비슷한 가사들이 생성될 것이다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "이번 프로젝트를 통해 인공지능이 문장을 이해하는 방식에 대해 알아볼 수 있었고 어떻게 모델을 학습시켜서 작문을 하게 하는지에 대한 전체적인 구조에 대해 이해할 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-special",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
