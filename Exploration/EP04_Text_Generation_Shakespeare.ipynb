{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97925dde",
   "metadata": {},
   "source": [
    "# Exploration 4. Text Generation  연극 대사 문장 생성\n",
    "\n",
    "\n",
    "\n",
    "순환신경망(RNN) 을 이용한 언어 모델로 연극의 대사를 학습해서 스스로 연극 대사 문장을 생성해내는 인공지능을 만들어보자.\n",
    "\n",
    "__셰익스피어와 버금가는 극작가 언어 모델__ 을 기대하며.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/84179578/126730385-cba700d8-37dc-4fd5-845a-9f81b7164719.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90d59a",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비\n",
    "\n",
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020d6003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "file_path = 'EP04_data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d1461e",
   "metadata": {},
   "source": [
    "데이터의 형태를 보면 아래와 같이 되어있다.\n",
    "![image](https://user-images.githubusercontent.com/84179578/126731288-a3c6b6e5-a98e-4a47-b763-aaee5cf9c0a1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e7923",
   "metadata": {},
   "source": [
    "여기서 필요한 문장은 대사인 부분이므로,  화자가 표기된 문장과 공백인 문장은 제외시킨다.   \n",
    "\n",
    "아래와 같이 제외시킬 문장의 특징을 찾아 문장을 제외시킨다.\n",
    "- 화자가 표기된 문장 : `:`로 끝나는 문장\n",
    "- 공백인 문장 : 문장의 길이를 검사하여 길이가 0 인 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33029ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장 건너뜀.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장 건너뜀.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b545c3c",
   "metadata": {},
   "source": [
    "### 토큰화 (Tokenize)\n",
    "다음과 같은 순서로 문장을 일정한 기준으로 쪼개는 __토큰화 (Tokenize)__ 를 진행한다.\n",
    "\n",
    "_정규표현식을 이용한 필터링이 유용하게 사용됨_   -> 정규표현식에 대한 자세한 내용은 아래 블로그 링크 참조\n",
    "> [AIFFEL FD #05 문자열, 파일 다루기 - 정규 표현식 (Regular expression, regex)](https://velog.io/@leejaejun/AIFFEL-FD-05-%EB%AC%B8%EC%9E%90%EC%97%B4-%ED%8C%8C%EC%9D%BC-%EB%8B%A4%EB%A3%A8%EA%B8%B0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fecccd",
   "metadata": {},
   "source": [
    "- 1. 소문자로 바꾸고, 양쪽 공백을 지움\n",
    "- 2. 특수문자 양쪽에 공백을 넣음\n",
    "- 3. 여러개의 공백은 하나의 공백으로 바꿈\n",
    "- 4. `a-zA-Z?.!,¿`가 아닌 모든 문자를 하나의 공백으로 바꿈\n",
    "- 5. 다시 양쪽 공백을 지움\n",
    "- 6. 문장 시작에는 `<start>`, 끝에는 `<end>`를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a25518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()                     # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)   # 4\n",
    "    sentence = sentence.strip()                             # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>'             # 6\n",
    "    return sentence\n",
    "\n",
    "# 샘플 문장 으로 잘되는지 테스트\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8417cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e465e7",
   "metadata": {},
   "source": [
    "토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용한다.  \n",
    "\n",
    "- `tf.keras.preprocessing.text.Tokenizer` 설명\n",
    "  - `num_words` : 7000단어를 기억할 수 있는 tokenizer 생성\n",
    "  - `filters` : 이전 단계에서 이미 문장을 정제했으니 filters가 필요하지않음\n",
    "  - `oov_token`: 7000단어에 포함되지 못한 단어는 `<unk>`로 변환\n",
    " \n",
    "  \n",
    "- `tf.keras.preprocessing.sequence.pad_sequences` 설명\n",
    "  - 입력 데이터의 시퀀스 길이를 일정하게 맞춰줌\n",
    "  - 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춤\n",
    "  - 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d94038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x0000022CA884F8C8>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8d6ed",
   "metadata": {},
   "source": [
    "생성된 텐서 데이터를 5번째행, 10 번째 열까지만 출력해서 형태를 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c34570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]\n",
      " [   2 1201    5 1201    5    3    0    0    0    0]\n",
      " [   2  199    4   11   92 1021  298   18 2314  513]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:5, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa0078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96927ae7",
   "metadata": {},
   "source": [
    "텐서 데이터는 모두 정수로 이루어져있다. 또한 모두 2번 인덱스로 시작하는데 2번 인덱스라 `<start>` 이기 때문인 것을 쉽게 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28dab47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # 소스문장 생성 \n",
    "\n",
    "tgt_input = tensor[:, 1:]  # 타켓 문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492ae14",
   "metadata": {},
   "source": [
    "tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성한다. 마지막 토큰은`<end>`가 아니라 `<pad>`일 가능성이 높습니다.\n",
    "\n",
    "tensor에서 `<start>`를 잘라내서 타겟 문장을 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e253c1",
   "metadata": {},
   "source": [
    "### 데이터셋 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a661330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70174a1",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8217fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae728b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[ 2.4839316e-04,  1.2015342e-04, -3.5721785e-04, ...,\n",
       "          2.8301118e-04,  1.4798935e-04, -3.4065481e-04],\n",
       "        [ 4.2692857e-04,  1.7370727e-04, -4.0230996e-04, ...,\n",
       "          3.1553474e-04,  2.6398891e-04, -1.9628822e-04],\n",
       "        [ 3.2791414e-04,  4.4448982e-04, -4.9103971e-04, ...,\n",
       "          4.9881014e-04,  2.8514100e-04, -2.8807076e-04],\n",
       "        ...,\n",
       "        [-6.1485905e-04,  1.7636900e-03, -2.8105821e-03, ...,\n",
       "         -1.6348425e-03, -2.4537423e-03,  1.6620897e-03],\n",
       "        [-5.3172838e-04,  2.0138898e-03, -2.9725763e-03, ...,\n",
       "         -2.1892895e-03, -2.5777128e-03,  1.9230701e-03],\n",
       "        [-4.4037620e-04,  2.2203559e-03, -3.0923714e-03, ...,\n",
       "         -2.6918913e-03, -2.6764076e-03,  2.1633166e-03]],\n",
       "\n",
       "       [[ 2.4839316e-04,  1.2015342e-04, -3.5721785e-04, ...,\n",
       "          2.8301118e-04,  1.4798935e-04, -3.4065481e-04],\n",
       "        [ 4.4742698e-04,  1.1057055e-04, -5.4323493e-04, ...,\n",
       "          1.8108700e-04, -2.3471417e-05, -3.7465498e-04],\n",
       "        [ 4.0549063e-04,  2.4129022e-04, -1.0639278e-03, ...,\n",
       "          2.5113695e-04, -1.8539622e-04, -5.1748462e-04],\n",
       "        ...,\n",
       "        [-3.3063459e-04,  1.5304955e-03, -2.6459491e-03, ...,\n",
       "         -2.3948755e-03, -1.5089799e-03,  6.7915657e-04],\n",
       "        [-3.7903039e-04,  1.7033187e-03, -2.8434384e-03, ...,\n",
       "         -2.8439320e-03, -1.7357263e-03,  1.0382195e-03],\n",
       "        [-3.9750617e-04,  1.8617022e-03, -2.9999397e-03, ...,\n",
       "         -3.2574460e-03, -1.9269472e-03,  1.3803481e-03]],\n",
       "\n",
       "       [[ 2.4839316e-04,  1.2015342e-04, -3.5721785e-04, ...,\n",
       "          2.8301118e-04,  1.4798935e-04, -3.4065481e-04],\n",
       "        [ 4.4695206e-04,  1.3862879e-04, -1.4257946e-04, ...,\n",
       "          7.5562409e-04,  2.7916447e-04, -7.9294341e-04],\n",
       "        [ 3.8397984e-04,  2.1165016e-04, -4.4360128e-04, ...,\n",
       "          1.1289129e-03,  3.2628578e-04, -1.1284885e-03],\n",
       "        ...,\n",
       "        [-2.3457746e-05,  2.3859059e-03, -2.6834602e-03, ...,\n",
       "         -3.3808746e-03, -1.8349923e-03,  1.7465429e-03],\n",
       "        [ 1.2436707e-05,  2.5017294e-03, -2.7753217e-03, ...,\n",
       "         -3.7278323e-03, -1.9878414e-03,  2.0412090e-03],\n",
       "        [ 5.7360507e-05,  2.5894772e-03, -2.8378635e-03, ...,\n",
       "         -4.0243734e-03, -2.1061164e-03,  2.3066886e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.4839316e-04,  1.2015342e-04, -3.5721785e-04, ...,\n",
       "          2.8301118e-04,  1.4798935e-04, -3.4065481e-04],\n",
       "        [ 4.1461029e-04,  3.3814713e-04, -8.7958365e-04, ...,\n",
       "          4.8865745e-04,  3.1916556e-05, -6.0358521e-04],\n",
       "        [ 4.4774398e-04,  4.5977824e-04, -8.8997366e-04, ...,\n",
       "          5.7528878e-04,  1.7293135e-04, -4.3672125e-04],\n",
       "        ...,\n",
       "        [ 8.9296227e-04,  2.3132865e-03, -3.1168552e-03, ...,\n",
       "         -2.7026939e-03, -9.8035869e-04,  1.2723269e-03],\n",
       "        [ 7.4823463e-04,  2.4531058e-03, -3.1810885e-03, ...,\n",
       "         -3.1645764e-03, -1.2799962e-03,  1.6526585e-03],\n",
       "        [ 6.3550886e-04,  2.5599748e-03, -3.2075606e-03, ...,\n",
       "         -3.5626616e-03, -1.5327540e-03,  1.9888016e-03]],\n",
       "\n",
       "       [[ 2.4839316e-04,  1.2015342e-04, -3.5721785e-04, ...,\n",
       "          2.8301118e-04,  1.4798935e-04, -3.4065481e-04],\n",
       "        [ 4.1461029e-04,  3.3814713e-04, -8.7958365e-04, ...,\n",
       "          4.8865745e-04,  3.1916556e-05, -6.0358521e-04],\n",
       "        [ 4.9495773e-04,  3.2894441e-04, -1.1108638e-03, ...,\n",
       "          9.1950718e-04,  5.3395046e-04, -3.2681052e-04],\n",
       "        ...,\n",
       "        [-3.6268926e-04,  2.0953207e-03, -2.1148219e-03, ...,\n",
       "         -2.2994741e-03, -9.3573553e-04,  1.7779921e-03],\n",
       "        [-3.5895311e-04,  2.3341700e-03, -2.3078301e-03, ...,\n",
       "         -2.8374763e-03, -1.3402061e-03,  2.1386750e-03],\n",
       "        [-3.3525680e-04,  2.5170529e-03, -2.4690055e-03, ...,\n",
       "         -3.3054235e-03, -1.6727212e-03,  2.4474123e-03]],\n",
       "\n",
       "       [[ 2.4839316e-04,  1.2015342e-04, -3.5721785e-04, ...,\n",
       "          2.8301118e-04,  1.4798935e-04, -3.4065481e-04],\n",
       "        [ 4.2172370e-04,  5.6521334e-05, -2.3884997e-04, ...,\n",
       "          6.6620501e-04,  7.3016790e-04, -9.4704170e-05],\n",
       "        [ 1.4189447e-04,  1.5672494e-04, -3.3306720e-04, ...,\n",
       "          3.3236051e-04,  1.0428721e-03,  2.9324490e-04],\n",
       "        ...,\n",
       "        [-3.6584126e-04,  2.0794370e-03, -3.3538595e-03, ...,\n",
       "         -2.0501190e-03, -1.5529729e-03,  7.8253588e-04],\n",
       "        [-3.8856815e-04,  2.2490697e-03, -3.4100590e-03, ...,\n",
       "         -2.5647483e-03, -1.8271657e-03,  1.2521337e-03],\n",
       "        [-3.8408395e-04,  2.3767448e-03, -3.4320666e-03, ...,\n",
       "         -3.0242929e-03, -2.0472838e-03,  1.6686933e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣음\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d12da7",
   "metadata": {},
   "source": [
    "모델의 최종 출력 텐서 shape 을 보면 `shape=(256,20,7001)` 임을 확인 할 수 잇다.\n",
    "각 숫자의 이미는 아래와 같다.\n",
    "- `256` : 이전 스텝에서 지정한 배치 사이즈  \n",
    "\n",
    "- `20` : `tf.keras.layers.LSTM(hidden_size, return_sequences=True)` 로 호출한 LSTM 레이어에서 `return_sequences=True`이라고 지정한 부분에 있습니다. 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미입니다. 만약 `return_sequences=False`였다면 LSTM 레이어는 1개의 벡터만 출력했을 것입니다.  \n",
    "\n",
    "- `7001` : Dense 레이어의 출력 차원수. 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8a9e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d47364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b209be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a91a01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9c6a336e424b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1150\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     self._adapter = adapter_cls(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m   \u001b[1;34m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mALL_ADAPTER_CLS\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[1;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m   \u001b[1;34m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mALL_ADAPTER_CLS\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[1;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mcan_handle\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[0mflat_inputs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m     \u001b[0mtensor_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tensor_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_get_tensor_types\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1633\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_tensor_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1635\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aiffel\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# hack but overkill to use re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aiffel\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[0;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85df79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557456b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> If\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a684551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
