{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numeric-briefs",
   "metadata": {},
   "source": [
    "# Exploration 4. Text Generation  연극 대사 문장 생성\n",
    "\n",
    "\n",
    "\n",
    "순환신경망(RNN) 을 이용한 언어 모델로 연극의 대사를 학습해서 스스로 연극 대사 문장을 생성해내는 인공지능을 만들어보자.\n",
    "\n",
    "__셰익스피어와 버금가는 극작가 언어 모델__ 을 기대하며.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/84179578/126730385-cba700d8-37dc-4fd5-845a-9f81b7164719.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-acquisition",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비\n",
    "\n",
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charged-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "file_path = 'EP04_data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-account",
   "metadata": {},
   "source": [
    "데이터의 형태를 보면 아래와 같이 되어있다.\n",
    "![image](https://user-images.githubusercontent.com/84179578/126731288-a3c6b6e5-a98e-4a47-b763-aaee5cf9c0a1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-handling",
   "metadata": {},
   "source": [
    "여기서 필요한 문장은 대사인 부분이므로,  화자가 표기된 문장과 공백인 문장은 제외시킨다.   \n",
    "\n",
    "아래와 같이 제외시킬 문장의 특징을 찾아 문장을 제외시킨다.\n",
    "- 화자가 표기된 문장 : `:`로 끝나는 문장\n",
    "- 공백인 문장 : 문장의 길이를 검사하여 길이가 0 인 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "devoted-above",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장 건너뜀.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장 건너뜀.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-norway",
   "metadata": {},
   "source": [
    "### 토큰화 (Tokenize)\n",
    "다음과 같은 순서로 문장을 일정한 기준으로 쪼개는 __토큰화 (Tokenize)__ 를 진행한다.\n",
    "\n",
    "_정규표현식을 이용한 필터링이 유용하게 사용됨_   -> 정규표현식에 대한 자세한 내용은 아래 블로그 링크 참조\n",
    "> [AIFFEL FD #05 문자열, 파일 다루기 - 정규 표현식 (Regular expression, regex)](https://velog.io/@leejaejun/AIFFEL-FD-05-%EB%AC%B8%EC%9E%90%EC%97%B4-%ED%8C%8C%EC%9D%BC-%EB%8B%A4%EB%A3%A8%EA%B8%B0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-division",
   "metadata": {},
   "source": [
    "- 1. 소문자로 바꾸고, 양쪽 공백을 지움\n",
    "- 2. 특수문자 양쪽에 공백을 넣음\n",
    "- 3. 여러개의 공백은 하나의 공백으로 바꿈\n",
    "- 4. `a-zA-Z?.!,¿`가 아닌 모든 문자를 하나의 공백으로 바꿈\n",
    "- 5. 다시 양쪽 공백을 지움\n",
    "- 6. 문장 시작에는 `<start>`, 끝에는 `<end>`를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continental-trading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()                     # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)   # 4\n",
    "    sentence = sentence.strip()                             # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>'             # 6\n",
    "    return sentence\n",
    "\n",
    "# 샘플 문장 으로 잘되는지 테스트\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "distinguished-northwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-penny",
   "metadata": {},
   "source": [
    "토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용한다.  \n",
    "\n",
    "- `tf.keras.preprocessing.text.Tokenizer` 설명\n",
    "  - `num_words` : 7000단어를 기억할 수 있는 tokenizer 생성\n",
    "  - `filters` : 이전 단계에서 이미 문장을 정제했으니 filters가 필요하지않음\n",
    "  - `oov_token`: 7000단어에 포함되지 못한 단어는 `<unk>`로 변환\n",
    " \n",
    "  \n",
    "- `tf.keras.preprocessing.sequence.pad_sequences` 설명\n",
    "  - 입력 데이터의 시퀀스 길이를 일정하게 맞춰줌\n",
    "  - 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춤\n",
    "  - 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brilliant-sunset",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f566a87c4d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-auditor",
   "metadata": {},
   "source": [
    "생성된 텐서 데이터를 5번째행, 10 번째 열까지만 출력해서 형태를 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complex-edwards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]\n",
      " [   2 1201    5 1201    5    3    0    0    0    0]\n",
      " [   2  199    4   11   92 1021  298   18 2314  513]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:5, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acting-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-favorite",
   "metadata": {},
   "source": [
    "텐서 데이터는 모두 정수로 이루어져있다. 또한 모두 2번 인덱스로 시작하는데 2번 인덱스라 `<start>` 이기 때문인 것을 쉽게 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aggressive-bruce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # 소스문장 생성 \n",
    "\n",
    "tgt_input = tensor[:, 1:]  # 타켓 문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-working",
   "metadata": {},
   "source": [
    "tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성한다. 마지막 토큰은`<end>`가 아니라 `<pad>`일 가능성이 높습니다.\n",
    "\n",
    "tensor에서 `<start>`를 잘라내서 타겟 문장을 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-consent",
   "metadata": {},
   "source": [
    "### 데이터셋 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "grateful-dairy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-vietnamese",
   "metadata": {},
   "source": [
    "## 2. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worthy-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "patent-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[ 1.29422348e-04, -8.74057805e-05, -3.90008150e-04, ...,\n",
       "         -2.27000874e-05,  1.71766192e-06, -1.82864344e-04],\n",
       "        [ 6.77151274e-06, -3.11454292e-04, -3.96353542e-04, ...,\n",
       "          1.45035694e-04,  1.65881938e-04, -4.09888016e-04],\n",
       "        [-6.95545386e-05, -6.02352375e-04, -4.32206551e-04, ...,\n",
       "          2.95698526e-04,  5.38130233e-04, -1.61567586e-04],\n",
       "        ...,\n",
       "        [-1.00875768e-05, -2.69636977e-04, -6.14988443e-04, ...,\n",
       "         -1.81736029e-03,  9.59836179e-04,  1.53853314e-03],\n",
       "        [-1.16229414e-04, -3.53646494e-04, -4.07907006e-04, ...,\n",
       "         -2.03736662e-03,  9.80988028e-04,  1.53697131e-03],\n",
       "        [-2.28689009e-04, -4.34681831e-04, -1.46642749e-04, ...,\n",
       "         -2.25878204e-03,  9.92755871e-04,  1.55003776e-03]],\n",
       "\n",
       "       [[ 1.29422348e-04, -8.74057805e-05, -3.90008150e-04, ...,\n",
       "         -2.27000874e-05,  1.71766192e-06, -1.82864344e-04],\n",
       "        [ 4.22317884e-04,  6.29304268e-05, -8.94410477e-04, ...,\n",
       "         -1.10053174e-04, -1.87215541e-04, -7.17040384e-05],\n",
       "        [ 5.15409920e-04,  1.39269512e-04, -1.51632854e-03, ...,\n",
       "         -1.34298927e-04, -1.85030614e-04, -4.74990593e-05],\n",
       "        ...,\n",
       "        [-6.74883311e-04,  3.04023433e-06, -3.42171523e-04, ...,\n",
       "         -1.97744812e-03,  5.27709548e-04,  1.00368541e-03],\n",
       "        [-7.88767182e-04, -2.21445283e-04, -1.13218608e-04, ...,\n",
       "         -2.25159456e-03,  5.23721101e-04,  1.13098882e-03],\n",
       "        [-8.97624181e-04, -4.18895070e-04,  1.63960402e-04, ...,\n",
       "         -2.50600046e-03,  5.28940582e-04,  1.25071045e-03]],\n",
       "\n",
       "       [[ 1.29422348e-04, -8.74057805e-05, -3.90008150e-04, ...,\n",
       "         -2.27000874e-05,  1.71766192e-06, -1.82864344e-04],\n",
       "        [ 4.28519095e-04, -3.62385908e-05, -3.25621018e-04, ...,\n",
       "         -3.22079381e-06,  8.04524680e-05, -3.01037129e-04],\n",
       "        [ 6.55584212e-04, -1.08008819e-04,  8.95029734e-05, ...,\n",
       "         -6.52117014e-05,  2.01189061e-04, -5.54298924e-04],\n",
       "        ...,\n",
       "        [-8.76773323e-04, -5.21781622e-04,  1.89167820e-03, ...,\n",
       "         -3.45417345e-03,  5.48935204e-04,  1.90303847e-03],\n",
       "        [-1.00516248e-03, -5.80991036e-04,  2.09179241e-03, ...,\n",
       "         -3.59373190e-03,  5.99745952e-04,  1.93359819e-03],\n",
       "        [-1.12604327e-03, -6.37016608e-04,  2.27728370e-03, ...,\n",
       "         -3.70962895e-03,  6.43565960e-04,  1.95331057e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.29422348e-04, -8.74057805e-05, -3.90008150e-04, ...,\n",
       "         -2.27000874e-05,  1.71766192e-06, -1.82864344e-04],\n",
       "        [ 2.72589357e-04, -1.70220097e-04, -4.37854353e-04, ...,\n",
       "         -1.37622301e-05,  2.68571050e-04, -8.99230305e-04],\n",
       "        [ 3.56058037e-04, -3.61337501e-04, -4.04222636e-04, ...,\n",
       "          9.16787030e-05,  6.08347647e-04, -8.62740970e-04],\n",
       "        ...,\n",
       "        [-1.27518189e-03, -4.28420899e-05, -5.45183138e-04, ...,\n",
       "         -2.85333954e-03, -8.43875925e-04,  9.04638786e-04],\n",
       "        [-1.35102740e-03, -1.11250847e-04, -2.76885054e-04, ...,\n",
       "         -3.02090612e-03, -6.31733274e-04,  1.06553303e-03],\n",
       "        [-1.41288631e-03, -1.97395799e-04,  3.99579076e-05, ...,\n",
       "         -3.17083765e-03, -4.24018595e-04,  1.21311343e-03]],\n",
       "\n",
       "       [[ 1.29422348e-04, -8.74057805e-05, -3.90008150e-04, ...,\n",
       "         -2.27000874e-05,  1.71766192e-06, -1.82864344e-04],\n",
       "        [-2.65792605e-05,  1.65820849e-04, -4.95685614e-04, ...,\n",
       "         -1.29985972e-04,  1.87566664e-04, -3.14812351e-04],\n",
       "        [-1.51468134e-06,  2.47286982e-04, -6.98660151e-04, ...,\n",
       "         -8.69520227e-05, -7.55274814e-05, -4.50972089e-04],\n",
       "        ...,\n",
       "        [-3.73689458e-04,  2.14609507e-04,  5.64364309e-04, ...,\n",
       "         -1.84814504e-03, -8.96211277e-05,  3.54662188e-04],\n",
       "        [-4.85908880e-04,  8.13212973e-05,  7.39502080e-04, ...,\n",
       "         -2.14934023e-03, -2.12589093e-05,  6.40912389e-04],\n",
       "        [-6.02266460e-04, -5.36501975e-05,  9.60956793e-04, ...,\n",
       "         -2.42154323e-03,  5.45556686e-05,  8.89972027e-04]],\n",
       "\n",
       "       [[ 1.29422348e-04, -8.74057805e-05, -3.90008150e-04, ...,\n",
       "         -2.27000874e-05,  1.71766192e-06, -1.82864344e-04],\n",
       "        [ 8.78379287e-05,  1.09870853e-05, -6.46176341e-04, ...,\n",
       "         -1.33553927e-04, -1.55643123e-04, -5.61862660e-04],\n",
       "        [-1.19795237e-04, -2.19702706e-04, -3.84297717e-04, ...,\n",
       "         -1.34379616e-05, -9.88115426e-05, -9.83573846e-04],\n",
       "        ...,\n",
       "        [-1.96616747e-03, -6.57789991e-04,  7.84851873e-05, ...,\n",
       "         -2.00803857e-03, -3.10541596e-04,  1.86096341e-03],\n",
       "        [-1.90035626e-03, -7.40633986e-04,  4.66172263e-04, ...,\n",
       "         -2.31185183e-03, -2.10651662e-04,  1.89531129e-03],\n",
       "        [-1.84908253e-03, -8.06364056e-04,  8.48878990e-04, ...,\n",
       "         -2.58271396e-03, -1.03982384e-04,  1.92530523e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣음\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-hampton",
   "metadata": {},
   "source": [
    "모델의 최종 출력 텐서 shape 을 보면 `shape=(256,20,7001)` 임을 확인 할 수 잇다.\n",
    "각 숫자의 이미는 아래와 같다.\n",
    "- `256` : 이전 스텝에서 지정한 배치 사이즈  \n",
    "\n",
    "- `20` : `tf.keras.layers.LSTM(hidden_size, return_sequences=True)` 로 호출한 LSTM 레이어에서 `return_sequences=True`이라고 지정한 부분에 있습니다. 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미입니다. 만약 `return_sequences=False`였다면 LSTM 레이어는 1개의 벡터만 출력했을 것입니다.  \n",
    "\n",
    "- `7001` : Dense 레이어의 출력 차원수. 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "demanding-shaft",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "resident-concrete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 38s 383ms/step - loss: 4.3701\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 36s 386ms/step - loss: 2.8164\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 36s 390ms/step - loss: 2.7129\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 36s 392ms/step - loss: 2.6057\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 37s 394ms/step - loss: 2.5288\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 37s 395ms/step - loss: 2.4758\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 37s 396ms/step - loss: 2.4225\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 37s 397ms/step - loss: 2.3634\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 37s 397ms/step - loss: 2.3072\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 37s 397ms/step - loss: 2.2621\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 37s 397ms/step - loss: 2.2075\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 2.1615\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 2.1079\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 2.0604\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 2.0107\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.9588\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.9081\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.8601\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.8049\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.7553\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.7041\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.6575\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.6122\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.5603\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.5035\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.4573\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.4038\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.3574\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.3001\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.2519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f566a74a9d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-intellectual",
   "metadata": {},
   "source": [
    "## 3. 모델 평가\n",
    "\n",
    "작문 모델을 평가하는 가장 확실한 방법은 __작문을 시켜보고 직접 평가하는 것__ 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "checked-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nutritional-leone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s not prepared for a man as it is , <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "simplified-shadow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i am not prone to <unk> , and will not <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "optical-homeless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she is not fourteen . <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adapted-advisory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> if you do wrongfully seize hereford s rights , <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> If\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-cycling",
   "metadata": {},
   "source": [
    "조금 어색하지만, 인공지능이 문장을 만들어내는것을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-browser",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
