{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "european-paste",
   "metadata": {},
   "source": [
    "# Exploration 10. Text Summarization - 추상적 요약(Abstractive Summarization)\n",
    "\n",
    "텍스트 요약(Text Summarization)이란 긴 길이의 문서(Document) 원문을 핵심 주제만으로 구성된 짧은 요약(Summary) 문장들로 변환하는 것이다.\n",
    "\n",
    "텍스트 요약은 크게 추출적 요약(extractive summarization)과 추상적 요약(abstractive summarization)으로 나뉜다.\n",
    "\n",
    "- __추출적 요약(Extractive Summarization)__ :  원문에서 중요한 핵심 문장 또는 단어구를 몇 개 뽑아서 이들로 구성된 요약문을 만드는 방법  \n",
    "\n",
    "\n",
    "- __추상적 요약(Abstractive Summarization)__ : 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법\n",
    "\n",
    "\n",
    "\n",
    "Kaggle 에서 제공된 `아마존 리뷰 데이터셋` 을 사용하여 추상적(Abstractive) 텍스트 요약 모델을 만들어보자.  \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/84179578/129665735-ea26a458-9ecd-48d5-a632-eedce24773fd.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-discretion",
   "metadata": {},
   "source": [
    "## 0. 필요한 모듈 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-malta",
   "metadata": {},
   "source": [
    "`NLTK` 는 Natural Language Toolkit의 축약어로 영어 기호, 통계, 자연어 처리를 위한 라이브러리이다. `NLTK` 를 이용하여 의미를 분석하고 요약하는 데는 거의 의미가 없는 불용어를 제거한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extreme-senior",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-worse",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비\n",
    "\n",
    "이번에 사용하는 데이터(Reviews.csv)는 총 568,454개의 샘플을 가지고있다. 시간상 여기서는 모든 샘플을 사용하지는 않고, 간단히 10만 개의 샘플만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "western-disposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "severe-letter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-spice",
   "metadata": {},
   "source": [
    "전체 데이터 중 훈련에 필요한 `Summary` 열과 `Text` 열만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "victorian-infrared",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary\n",
       "0  I have bought several of the Vitality canned d...  Good Quality Dog Food\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      Not as Advertised\n",
       "2  This is a confection that has been around a fe...  \"Delight\" says it all\n",
       "3  If you are looking for the secret ingredient i...         Cough Medicine\n",
       "4  Great taffy at a great price.  There was a wid...            Great taffy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Text','Summary']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-suspension",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-healing",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "데이터 전처리 과정은 크게 세 과정으로 나눠서 진행한다.\n",
    "- 데이터 정리\n",
    "- 훈련 데이터와 데스트 데이터 분할\n",
    "- 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-steering",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 -  데이터 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-commons",
   "metadata": {},
   "source": [
    "### 중복 샘플과 NULL 값이 존재하는 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applicable-defeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 중복 샘플 유무 확인\n",
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-gauge",
   "metadata": {},
   "source": [
    "`Summary`는 아주 간단한 요약들도 많아서 `Text`가 달라도 `Summary`는 동일할 수 있다. 따라서 `Text` 자체가 중복이 된 경우만 중복 샘플로 취급하여 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electoral-ottawa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿈\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-hollywood",
   "metadata": {},
   "source": [
    "`.isnull().sum()` 를 사용해 데이터의 Null 값이 있는지 확인하고 `dropna()` 를 사용해 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alleged-journalist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sufficient-proof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-sympathy",
   "metadata": {},
   "source": [
    "### 텍스트 정규화와 불용어 제거\n",
    "\n",
    "__텍스트 정규화(text normalization)__ 란, `it'll` = `it will`, `mustn't` = `must not` 와 같은 같은 의미의 표현들을 통일시키는 과정이다. 텍스트 정규화를 통해 기계의 연산량을 줄일 수 있다.  \n",
    "\n",
    "아래의 링크를 참고하여 텍스트 정규화를 위한 딕셔너리를 구성한다.  \n",
    "> [정규화 사전 출처](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polyphonic-animal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-installation",
   "metadata": {},
   "source": [
    "불용어란 자연어 처리를 할 때 의미를 분석하고 요약하는 데는 거의 의미가 없는 단어이다.  \n",
    "\n",
    "`NLTK`에서 제공하는 불용어 리스트를 참조해, 샘플에서 불용어를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "smooth-syndicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-prime",
   "metadata": {},
   "source": [
    "다음과 같은 기능을 수행하는 데이터 전처리 함수 `preprocess_sentence` 를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-result",
   "metadata": {},
   "source": [
    "- 모든 영어 문자를 소문자로 변환\n",
    "- 섞여있는 html 태그 제러\n",
    "- 정규 표현식을 통해 각종 특수문자 제거\n",
    "- `Text` 데이터만 불용어 제거하기 위해 함수의 인자로 `remove_stopwords` 추가 -> Abstractive한 문장 요약 결과문이 자연스러운 문장이 되려면 이 불용어들이 Summary에는 남아 있는 게 더 좋음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "outer-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "considered-mainland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered wasfor mother father\n",
      "great way to start the day\n"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-recipient",
   "metadata": {},
   "source": [
    "훈련 데이터 전체에 대해서 전처리를 수행한다.  \n",
    "\n",
    "`Text`의 경우에는 불용어를 제거하고, `Summary` 의 경우에는 불용어를 제거하지 않을 것이므로 따로 호출해서 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-process",
   "metadata": {},
   "source": [
    "```python\n",
    "clean_text = []\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(clean_text[:5])\n",
    "\n",
    "\n",
    "clean_summary = []\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(clean_summary[:5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-brisbane",
   "metadata": {},
   "source": [
    "위 코드와 같이 싱글 프로세스로 실행하면 데이터 전처리 하는데 꽤나 많은 시간이 소요된다.  \n",
    "\n",
    "따라서 멀티프로세싱을 활용하여 별도의 프로세스를 생성하여 병렬처리하면 CPU수에 비례하여 획기적으로 소요 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "critical-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229.71000266075134  seconds\n",
      "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better'\n",
      " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo'\n",
      " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch'\n",
      " ...\n",
      " 'favorite brand korean ramen spicy used eating spicy food make sure use spice pack add egg soup makes great snack'\n",
      " 'like noodles although say spicy somewhat understatement one else family tolerates spicy well seeing looking forward extra little something palate disappointed completely honest usually drain liquid almost much'\n",
      " 'love noodle twice week amazing thing feel well cold hot bowl noodle cure upset stomach headache running nose may work definitely try']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1818037033081055  seconds\n",
      "['good quality dog food' 'not as advertised' 'delight says it all' ...\n",
      " 'great ramen' 'spicy'\n",
      " 'this spicy noodle cures my cold upset stomach and headache every time']\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp   # 멀티 프로세싱으로 전처리 속도를 획기적으로 줄여봅시다\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial  # map을 할 때 함수에 여러 인자를 넣어줄 수 있도록 합니다\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# num_cores 만큼 쪼개진 데이터를 전처리하여 반환합니다\n",
    "def appendTexts(sentences, remove_stopwords):\n",
    "  texts = []\n",
    "  for s in sentences:\n",
    "    texts += preprocess_sentence(s, remove_stopwords),\n",
    "  return texts\n",
    "\n",
    "def preprocess_data(data, remove_stopwords=True):\n",
    "  start_time = time.time()\n",
    "  num_cores = mp.cpu_count()  # 컴퓨터의 코어 수를 구합니다\n",
    "\n",
    "  text_data_split = np.array_split(data, num_cores)  # 코어 수만큼 데이터를 배분하여 병렬적으로 처리할 수 있게 합니다\n",
    "  pool = Pool(num_cores)\n",
    "\n",
    "  processed_data = np.concatenate(pool.map(partial(appendTexts, remove_stopwords=remove_stopwords), text_data_split))  # 각자 작업한 데이터를 하나로 합쳐줍니다\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  print(time.time() - start_time, \" seconds\")\n",
    "  return processed_data\n",
    "\n",
    "clean_text = preprocess_data(data['Text'])  # 클라우드 기준으로 3~4분 정도 소요 됩니다\n",
    "print(clean_text)\n",
    "\n",
    "clean_summary = preprocess_data(data['Summary'], remove_stopwords=False) # 클라우드 기준 1분정도 소요됩니다.\n",
    "print(clean_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-diversity",
   "metadata": {},
   "source": [
    "이렇게 텍스트 정제의 과정을 거친 후에는 다시 한번 빈(empty) 샘플이 생겼는지 확인해보는 것이 좋다.  \n",
    "\n",
    "정제 전에는 데이터가 존재했지만, 정제 과정에서 문장의 모든 단어가 사라지는 경우가 있을 수 있디. 이렇게 되면 샘플 자체가 빈 값을 가지게 된다.  \n",
    "\n",
    "\n",
    "데이터를 보다 쉽게 확인하기 위하여 데이터프레임에 저장한다. 이후 빈(empty) 값을 가진 샘플들이 있다면, 모두 Null 값을 가진 샘플로 대체한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "endless-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beginning-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-repeat",
   "metadata": {},
   "source": [
    "`Summary` 열에서 70개의 Null 값이 생겼고  이것을 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "present-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-stretch",
   "metadata": {},
   "source": [
    "----\n",
    "## 3. 데이터 전처리 - 훈련 데이터와 테스트 데이터 분할 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-building",
   "metadata": {},
   "source": [
    "### 샘플의 최대 길이 정하기\n",
    "\n",
    "Text와 Summary의 최소, 최대, 평균 길이를 구하고 또한 길이 분포를 시각화해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "elect-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-traveler",
   "metadata": {},
   "source": [
    "위에서부터 그래프의 의미는  차례대로 아래과 같다.\n",
    "- `Summary` 과 실제 `Text` 의 길이 분포\n",
    "- 요약본 샘플 길이별 개수\n",
    "- 실제 텍스트 샘플 길이별 개수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-accreditation",
   "metadata": {},
   "source": [
    "`Text`의 경우 최소 길이가 2, 최대 길이가 1,235로 그 차이가 굉장히 크다. 하지만 평균 길이는 38로 시각화된 그래프로 봤을 때는 대체적으로는 100 내외의 길이를 가진다는 것을 확인 할 수 있다.  \n",
    "\n",
    "`Summary`의 경우 최소 길이가 1, 최대 길이가 28, 그리고 평균 길이가 4로 Text에 비해 상대적으로 길이가 매우 짧다. 그래프로 봤을 때에도 대체적으로 10이하의 길이를 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-anxiety",
   "metadata": {},
   "source": [
    "이로부터 Text의 최대 길이와 Summary의 적절한 최대 길이를 임의로 정하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "neutral-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-supervision",
   "metadata": {},
   "source": [
    "훈련 데이터와 샘플의 길이를 입력하면, 데이터의 몇 %가 해당하는지 계산하는 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "amino-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "brave-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-robertson",
   "metadata": {},
   "source": [
    "정해진 길이보다 길면 제외하는 방법으로 데이터를 정제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "accepting-sweet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65818\n"
     ]
    }
   ],
   "source": [
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-sussex",
   "metadata": {},
   "source": [
    "### 시작 토큰과 종료 토큰 추가\n",
    "seq2seq 훈련을 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가할 필요가 있다.  \n",
    "\n",
    "시작 토큰은 `sostoken`, 종료 토큰은 `eostoken`이라 임의로 명명하고 앞, 뒤로 추가한다.  \n",
    "\n",
    "또한, 디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장의 이름을 `decoder_input`, \n",
    "디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장의 이름을 `decoder_target`이라고 명명하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rotary-alcohol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "proud-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더의 입력, 디코더의 입력과 레이블을 각각 다시 Numpy 타입으로 저장 \n",
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-wales",
   "metadata": {},
   "source": [
    "분리 패키지를 사용해서 훈련 데이터와 테스트 데이터를 분리 할 수도 있지만 이번에는 직접 코딩을 통해서 분리해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-sensitivity",
   "metadata": {},
   "source": [
    "#### 순서 섞기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "textile-welsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11866 64461 45191 ... 38959 38501 31709]\n"
     ]
    }
   ],
   "source": [
    "# encoder_input과 크기와 형태가 같지만 순서가 섞인 정수 시퀀스\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "parallel-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 정수 시퀀스를 이용해 다시 데이터의 샘플 순서를 정의\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-paraguay",
   "metadata": {},
   "source": [
    "#### 섞인 데이터를 8:2의 비율로 훈련 데이터와 테스트 데이터로 분리\n",
    "\n",
    "전체 데이터의 크기에서 0.2를 곱해서 테스트 데이터의 크기를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "continent-cheese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "monetary-present",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-jersey",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 4. 데이터 전처리 - 정수 인코딩\n",
    "### 단어 집합(vocabulary) 만들기 및 정수 인코딩\n",
    "\n",
    "기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 고유한 정수로 맵핑하는 적업이 필요하다.  이 과정을 단어 __집합(vocabulary)__ 을 만든다고 표현한다.  \n",
    "\n",
    "Keras의 토크나이저를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "casual-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-possession",
   "metadata": {},
   "source": [
    " 현재 생성된 단어 집합은 `src_tokenizer.word_index` 에 저장되어 있다.  \n",
    " \n",
    " \n",
    " 만든 단어 집합에 있는 모든 단어를 사용하는 것이 아니라, 빈도수가 낮은 단어들은 훈련 데이터에서 제외한다.  \n",
    " \n",
    " `src_tokenizer.word_counts.items()` 를 사용하여 등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hairy-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 32040\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23785\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8255\n",
      "단어 집합에서 희귀 단어의 비율: 74.23533083645442\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.3933633024013297\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-speed",
   "metadata": {},
   "source": [
    "- 등장 빈도가 threshold 값인 7회 미만, 즉, 6회 이하인 단어들은 단어 집합에서 무려 70% 이상을 차지함. \n",
    "- 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%밖에 되지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-bracelet",
   "metadata": {},
   "source": [
    "그래서 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거한다.  \n",
    "\n",
    "위에서 이를 제외한 단어 집합의 크기를 8천여 개로 계산했는데, 이와 비슷한 값으로 어림잡아 단어 집합의 크기를 8,000으로 제한한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "animated-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-password",
   "metadata": {},
   "source": [
    "`texts_to_sequences()`는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행한다.  \n",
    "\n",
    "현재 단어 집합의 크기를 8,000으로 제한했으니까 이제 8,000이 넘는 숫자들은 정수 인코딩 후에는 데이터에 존재하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bigger-thunder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 15, 111, 8, 532, 59, 110, 1177, 237, 40, 83, 38, 4272, 14]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-company",
   "metadata": {},
   "source": [
    "__`Summary`__ 데이터에 대해서도 동일한 작업을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hungry-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 집합과 각 단어에 대한 빈도수를 계산\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "plastic-walter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10500\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8127\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2373\n",
      "단어 집합에서 희귀 단어의 비율: 77.4\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.901323126073722\n"
     ]
    }
   ],
   "source": [
    "# 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인\n",
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deadly-sudan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1], [1, 516, 864], [1, 32, 1022, 13, 140], [1, 257, 1801, 7], [1, 114, 305, 198]]\n",
      "target\n",
      "decoder  [[2], [516, 864, 2], [32, 1022, 13, 140, 2], [257, 1801, 7, 2], [114, 305, 198, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 2,000을 단어 집합의 크기로 제한\n",
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-signature",
   "metadata": {},
   "source": [
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었을 가능성이 있다.  \n",
    "\n",
    "decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태이므로 길이가 0이 된 요악문의 실제 길이는 1로 나올 것이다. \n",
    "\n",
    "훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 각각 `drop_train`과 `drop_test`에 라는 변수에 저장하고 샘플들은 모두 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "revised-uncle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1260\n",
      "삭제할 테스트 데이터의 개수 : 313\n",
      "훈련 데이터의 개수 : 51395\n",
      "훈련 레이블의 개수 : 51395\n",
      "테스트 데이터의 개수 : 12850\n",
      "테스트 레이블의 개수 : 12850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-marking",
   "metadata": {},
   "source": [
    "### 패딩하기\n",
    "\n",
    "최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 아까 정해두었던 최대 길이로 길이를 맞춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "military-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-blood",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 5. 모델 설계\n",
    "\n",
    "함수형 API 를 이용해서 인코더를 설계한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "moral-pakistan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-anniversary",
   "metadata": {},
   "source": [
    "인코더의 LSTM은 총 3개의 층으로 구성해서 모델의 복잡도를 높였다. 3개의 층을 지나서 인코더로부터 나온 출력 벡터는 디코더로 보내야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "duplicate-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-tomato",
   "metadata": {},
   "source": [
    "디코더의 임베딩 층과 LSTM을 설계하는 것은 인코더와 거의 동일하다.   \n",
    "\n",
    "하지만 LSTM의 입력을 정의할 때, `initial_state`의 인자값으로 인코더의 hidden state와 cell state의 값을 넣어줘야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "capable-graduate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층 설계\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-greeting",
   "metadata": {},
   "source": [
    "디코더의 출력층에서는 Summary의 단어장인 tar_vocab의 수많은 선택지 중 하나의 단어를 선택하는 다중 클래스 분류 문제를 풀어야 한다.  \n",
    "그렇기 때문에 Dense의 인자로 `tar_vocab`을 주고, 활성화 함수로 소프트맥스 함수를 사용하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-pound",
   "metadata": {},
   "source": [
    "### 어텐션 메커니즘\n",
    "\n",
    "지금까지 설계한 것은 인코더의 hidden state와 cell state를 디코더의 초기 state로 사용하는 가장 기본적인 seq2seq 이다.   \n",
    "\n",
    "디코더의 출력층의 설계를 수정해서 성능을 높일 수 있는 방법인 어텐션 메커니즘을 사용해보자.  \n",
    "\n",
    "깃허브에 공개되어있는 어텐션 함수를 다운로드하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "macro-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "mighty-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,276,432\n",
      "Trainable params: 4,276,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-dream",
   "metadata": {},
   "source": [
    "위의 코드는 인코더의 hidden state들과 디코더의 hidden state들을 어텐션 함수의 입력으로 사용하고, 어텐션 함수가 리턴한 값을 예측 시에 디코더의 hidden state와 함께 활용하는 형태로 작동한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-rouge",
   "metadata": {},
   "source": [
    "----\n",
    "## 6. 모델 훈련\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "flexible-cooperative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "201/201 [==============================] - 148s 678ms/step - loss: 3.1118 - val_loss: 2.4158\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 135s 669ms/step - loss: 2.4095 - val_loss: 2.2770\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 135s 671ms/step - loss: 2.2708 - val_loss: 2.1531\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 135s 670ms/step - loss: 2.1301 - val_loss: 2.0742\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 134s 669ms/step - loss: 2.0428 - val_loss: 1.9978\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 135s 670ms/step - loss: 1.9628 - val_loss: 1.9682\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 134s 669ms/step - loss: 1.9102 - val_loss: 1.9305\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 135s 670ms/step - loss: 1.8595 - val_loss: 1.9096\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 135s 669ms/step - loss: 1.8186 - val_loss: 1.8934\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 133s 662ms/step - loss: 1.7847 - val_loss: 1.8747\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 132s 657ms/step - loss: 1.7539 - val_loss: 1.8715\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 131s 653ms/step - loss: 1.7124 - val_loss: 1.8598\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 130s 647ms/step - loss: 1.6766 - val_loss: 1.8505\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 128s 638ms/step - loss: 1.6511 - val_loss: 1.8515\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.6242 - val_loss: 1.8423\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 128s 638ms/step - loss: 1.6064 - val_loss: 1.8420\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.5829 - val_loss: 1.8403\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 128s 639ms/step - loss: 1.5530 - val_loss: 1.8390\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 129s 641ms/step - loss: 1.5304 - val_loss: 1.8407\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.5194 - val_loss: 1.8418\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-disability",
   "metadata": {},
   "source": [
    "조기 종료'를 뜻하는 `EarlyStopping` 은 특정 조건이 충족되면 훈련을 멈추는 역할을 한다.  \n",
    "\n",
    "위 코드에서는 val_loss(검증 데이터의 손실)을 관찰하다가, 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 2회(patience=2) 관측되면 학습을 멈추도록 설정했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-electron",
   "metadata": {},
   "source": [
    "훈련 데이터의 손실과 검증 데이터의 손실이 줄어드는 과정을 시각화 하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "social-chaos",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-caution",
   "metadata": {},
   "source": [
    "## 7. 인퍼런스 모델 구현\n",
    "테스트 단계에서는 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 필요한 3개의 사전을 아래와 같이 미리 준비해둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "european-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-producer",
   "metadata": {},
   "source": [
    "seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해야한다.  \n",
    "\n",
    "\n",
    "훈련 단계에서는 디코더의 입력부에 정답이 되는 문장 전체를 한꺼번에 넣고 디코더의 출력과 한 번에 비교할 수 있으므로, 인코더와 디코더를 엮은 통짜 모델 하나만 준비했다.  \n",
    "\n",
    "그러나 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 인코더 모델과 디코더 모델을 분리해서 설계한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "statutory-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "arctic-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-alliance",
   "metadata": {},
   "source": [
    "인퍼런스 단계에서 단어 시퀀스를 완성하는 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "wicked-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-microphone",
   "metadata": {},
   "source": [
    "## 8. 모델 테스트\n",
    "\n",
    "주어진 정수 시퀀스를 텍스트 시퀀스로 변환하는 함수를 만든다.  \n",
    "\n",
    "이때, Text의 정수 시퀀스에서는 패딩을 위해 사용되는 숫자 0을 제외하고 Summary의 정수 시퀀스에서는 숫자 0, 시작 토큰의 인덱스, 종료 토큰의 인덱스를 출력에서 제외하도록 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "metric-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-complexity",
   "metadata": {},
   "source": [
    "테스트 데이터 약 50개의 샘플에 대해서 실제 요약과 예측된 요약을 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "extraordinary-student",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : product made excellent quality ingredients packaging vanilla added gave biscotti sweet rolls cakes best taste ever would highly recommend product anyone bakes delicious \n",
      "실제 요약 : yummy \n",
      "예측 요약 :  best seasoning\n",
      "\n",
      "\n",
      "원문 : shake bake popular coating loved years packet provides crunchy flavorful coating pork chops generally use boneless tried think pork chops also like taste feel crunchy coating keeps flavor leftovers makes quick run go snack tasty \n",
      "실제 요약 : nice and crispy \n",
      "예측 요약 :  great for quick meal\n",
      "\n",
      "\n",
      "원문 : two cats love cat food prefer eating dry cat food wet food says lot \n",
      "실제 요약 : best cat food \n",
      "예측 요약 :  my cats love it\n",
      "\n",
      "\n",
      "원문 : ordered tea amazon got pack without trying glad tea smells tastes good never mint tea chamomile tea mix two tea strong flavor either one highly recommend thing one tea bag good big cup tea strong making second cup \n",
      "실제 요약 : very nice flavor \n",
      "예측 요약 :  good tea\n",
      "\n",
      "\n",
      "원문 : used crystal light products mixed reviews ounce water bottle great many ways refuse buy bottled water anymore waste plastic get great bottles make ounce bottle diet lemon iced tea crystal light iced tea go lemon overpowering quite refreshing know reviews one deal enjoy \n",
      "실제 요약 : makes life so easy \n",
      "예측 요약 :  good stuff\n",
      "\n",
      "\n",
      "원문 : found pork rinds several years ago love believe first would really pop microwave like pop corn great get eat warm instead bag \n",
      "실제 요약 : pork rinds \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : dog lives treats cannot wait see us leave morning give \n",
      "실제 요약 : great treats \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : bought alot cup reason box cups explode like pressure cup good never order amazon \n",
      "실제 요약 : cup \n",
      "예측 요약 :  keurig coffee\n",
      "\n",
      "\n",
      "원문 : enjoyed tea past months cup morning little milk sugar recently seems store shelves cannot find anywhere except online looking replacement trying celestial chai good earth chai tazo strong hopefully find one comes close looks like lipton spiced chai soon discontinued item sad \n",
      "실제 요약 : great flavor but seems to be \n",
      "예측 요약 :  too strong for me\n",
      "\n",
      "\n",
      "원문 : first remarkable tea strong worth price thing wrong wrapping cabinet boxes tea completely dirty tea grains somehow bits pieces fall box plastic wrapping complaint \n",
      "실제 요약 : only thing wrong \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : looking really bold cup coffee must try cups actually brewing like great cup espresso husband looking great morning cup stick donut shop cup market really true rich strong cup go \n",
      "실제 요약 : wow \n",
      "예측 요약 :  excellent coffee\n",
      "\n",
      "\n",
      "원문 : love product read another comment posted another oil buying perfect hair love love love \n",
      "실제 요약 : extremely amazing \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : find coffee good better top dollar coffees always rich bold flavor like strong coffee great taste \n",
      "실제 요약 : coffee \n",
      "예측 요약 :  good coffee\n",
      "\n",
      "\n",
      "원문 : ordered several times time everything comes time course berries wonderful \n",
      "실제 요약 : berries \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : gave tully kona blend four stars expensive great coffee due price think coffee good lot cheaper mind cost getting super nice coffee strong taste bitterness always mug one cup maker oz sizes gives mug full amazon still best place buy coffee last time checked prices \n",
      "실제 요약 : in your cup \n",
      "예측 요약 :  good coffee\n",
      "\n",
      "\n",
      "원문 : kitchens india curry paste become staple item pantry home allows fast meals slightly meals weekend \n",
      "실제 요약 : the best \n",
      "예측 요약 :  great for cooking\n",
      "\n",
      "\n",
      "원문 : wasa great line crackers become favorite light wimpy flavor fairly neutral whatever choose put calorie count final winner calories per cracker low sodium winner way around amazon absolute best price \n",
      "실제 요약 : up to it is name \n",
      "예측 요약 :  great for cooking\n",
      "\n",
      "\n",
      "원문 : coconut oil best organic affordable coconut oil found love baking cooking body moisturizer smells heavenly \n",
      "실제 요약 : wonderful \n",
      "예측 요약 :  best olive oil\n",
      "\n",
      "\n",
      "원문 : would buy run lb lot hibiscus great making herbal tea great adding zest cocktails go well many things \n",
      "실제 요약 : simply superb \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : plug nose chug stuff disgusting pour glass drink taste stuff able take another gulp forget next day get coconut water naked juice way better \n",
      "실제 요약 : gross \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : like says clusters oats without flakes nice change honey bunches oats nice thing add original think enough clusters unlike original makes nice snack either way yummy \n",
      "실제 요약 : yummy \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : addicted bars years buying local supermarket breaking square day treat orange flavoring complements creamy medium cacao chocolate wonderfully orange chocolate fan melt square mouth enjoy find orange flavoring superior organic chocolate bars orange flavor bits orange zest want truly dark chocolate taste look elsewhere though \n",
      "실제 요약 : addicting if you the orange chocolate combination \n",
      "예측 요약 :  chocolate\n",
      "\n",
      "\n",
      "원문 : good alternative coconut oil hair usually use coconut oil occasionally use oil long hair well washes easily \n",
      "실제 요약 : nice hair oil \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : oatmeal cups great working person convenient taste great better drive thru fast food breakfast keep office days running late time eat think little pricy guess paying convenience \n",
      "실제 요약 : great breakfast \n",
      "예측 요약 :  great for\n",
      "\n",
      "\n",
      "원문 : perfect portion control like stand bag great putting candy fridge opposed flat bags husband trying lost weight able grab one calorie chocolate packs great also pleased shipped pay little packs fine came styrofoam box keep chocolate melting definitely order amazon \n",
      "실제 요약 : perfect size \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : price first purchased jumped stopped buying price coffee good good \n",
      "실제 요약 : organic morning ground coffee ounce \n",
      "예측 요약 :  good coffee\n",
      "\n",
      "\n",
      "원문 : sugar awesome bought hubby uses way less type sugar vs regular granulated sugar find store local stores organic sugars none like bakes well melts good tastes great ice tea cannot go wrong \n",
      "실제 요약 : yum \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : youngest loves flavor straight older kids enjoy without even knowing youngest often takes lunchbox preschool enjoy day since many times would like older kids favorite product hide treats apple cherry flavoring delicious brownies cupcakes makes everything healthier \n",
      "실제 요약 : it in \n",
      "예측 요약 :  great for kids\n",
      "\n",
      "\n",
      "원문 : use product every time shower great usually get horribly dry skin winter along dove lotion keeps skin soft smooth smell good overwhelming get alot product bottle huge comparable department store body washes except lot affordable \n",
      "실제 요약 : skin great smell \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : delicious dense wheat free bread heavy american style bread best toasted yummy cheese melted \n",
      "실제 요약 : just what was looking for \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : cure drinking much coffee morning get one thought first sugar spoon mr clean cloth next day one cup tully made second cup green mountain breakfast blend taste back would buy one \n",
      "실제 요약 : aftertaste \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : baked batch kale chips sprinkled salt vinegar seasoning addictive good balance salt vinegar flavors also sprinkled smoked almonds macadamia nuts good seasoning hard time sticking nuts overall tasty low carb way season foods great snacks \n",
      "실제 요약 : excellent on chips and nuts \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : discovered pg tips england best black tea used drink lipton red rose every day run pg skip tea day \n",
      "실제 요약 : best black tea \n",
      "예측 요약 :  best tea ever\n",
      "\n",
      "\n",
      "원문 : best stroganoff since natural touch use crumbles make vegetarian could use noodles \n",
      "실제 요약 : love it \n",
      "예측 요약 :  great for soups\n",
      "\n",
      "\n",
      "원문 : gevalia far best coffee market especially peaberry peruvian organic \n",
      "실제 요약 : gevalia is the best \n",
      "예측 요약 :  best coffee\n",
      "\n",
      "\n",
      "원문 : soda rocks welcome change pepsi coke world taste sweet great got orange color right nose drink much stuff turn lips teeth orange overall great change pepsi coke cola great taste give soda \n",
      "실제 요약 : review \n",
      "예측 요약 :  great soda\n",
      "\n",
      "\n",
      "원문 : happy discovered product tried several brands extract powder form far best virtually aftertaste quick dissolving convenient also economical cents serving use teaspoon measuring spoon sweetens coffee tea perfectly flavor get stevia powder happy \n",
      "실제 요약 : the best have ever had \n",
      "예측 요약 :  great for cooking\n",
      "\n",
      "\n",
      "원문 : children eat gluten free love pasta great deal use tinkyada cheese slices little almond milk melted hot noodles make macaroni cheese occasionally \n",
      "실제 요약 : my kids love this \n",
      "예측 요약 :  great pasta\n",
      "\n",
      "\n",
      "원문 : started getting vet treats bichon king charles mix spent days hospital death door pound well yellow lab get excited time daily treat treats several months without problems \n",
      "실제 요약 : our dogs love these chews \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  great for dogs with allergies\n",
      "\n",
      "\n",
      "원문 : weight watchers chewy trail mix flavor one keep completely satisfied meals love promise points per bar healthy way lose weight \n",
      "실제 요약 : awesome filling granola bars \n",
      "예측 요약 :  great for lunch\n",
      "\n",
      "\n",
      "원문 : best bread ever tasted tried best artisan breads hands better cheaper artisan breads tried flavor full character texture never gives feeling bread real nutrients \n",
      "실제 요약 : good \n",
      "예측 요약 :  best gf bread mix\n",
      "\n",
      "\n",
      "원문 : love apple chips eat every day make great low calorie snack bought costco stock amazon subscription easy price arrive automatically every month convenient service \n",
      "실제 요약 : baked dried apples \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : pretty tasty orange never favorite recommend like flavored drinks absolutely love strawberry kiwi drinks sweet natural refreshing continue try beverage choice water milk treat drinks like great try \n",
      "실제 요약 : tasty \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : cute idea would made great christmas gift pretzels million pieces dozen recognizable christmas shape needs way better packing guess making lot pretzel chicken \n",
      "실제 요약 : cute idea but \n",
      "예측 요약 :  fun\n",
      "\n",
      "\n",
      "원문 : find cereal good plain like grocery stores around carry particular type bran anymore idea like plain always add bananas berries eat every morning \n",
      "실제 요약 : delicious plain fiber \n",
      "예측 요약 :  great cereal\n",
      "\n",
      "\n",
      "원문 : product little white chocolate tastes like cardboard tasted one bar threw nine \n",
      "실제 요약 : tastes like cardboard \n",
      "예측 요약 :  not as good as the chocolate\n",
      "\n",
      "\n",
      "원문 : bars great alternative granola bars cannot eat gluten filling little sweeter delicious \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  great bar\n",
      "\n",
      "\n",
      "원문 : good quality popcorn tastes great pops fast half cup little oil make big pot corn much else say really came packaged well \n",
      "실제 요약 : tastes great \n",
      "예측 요약 :  great popcorn\n",
      "\n",
      "\n",
      "원문 : spices products great live northern area big stores specialty shops wonderful able fill spice shelf every thing need fabulous customer service \n",
      "실제 요약 : this is an company \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : great product flavors delicious loved fact single serve bags calories \n",
      "실제 요약 : yummy \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-showcase",
   "metadata": {},
   "source": [
    "많은 결과가 출력이 되는데, 기존의 요약과는 다른 요약을 출력하면서도 원문의 내용을 담고 있는 의미 있는 요약들이 보인다.  심지어 일부 요약의 경우에는 원문에 없던 단어를 사용해서 요약을 하기도 하고 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-container",
   "metadata": {},
   "source": [
    "모델의 성능을 개선하기 위해서는 seq2seq와 어텐션의 자체의 조합을 좀 더 좋게 수정하는 방법도 있고, 빔 서치(beam search), 사전 훈련된 워드 임베딩(pre-trained word embedding), 또는 인코더 - 디코더 자체의 구조를 새로이 변경한 하는 트랜스포머(Transformer)와 같은 여러 개선 방안들이 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-disability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
